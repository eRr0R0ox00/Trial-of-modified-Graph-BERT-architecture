It is a modified Graph-Transformer architecture.
This model applies Laplacian Positional Encoding instead of traditional positional encoding to adapt graph structure. And multihead-attention is replaced by GAT to attempt to adjust graph structure further.
